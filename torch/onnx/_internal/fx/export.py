import copy
import inspect
import itertools
import operator
from typing import Callable, Dict, Optional, Tuple, Union

import onnx
import onnxscript
from onnxscript import evaluator, opset18
from onnxscript.function_libs.torch_aten import graph_building, ops

import torch
import torch._C
import torch._decomp
import torch._dynamo
import torch._ops
from torch._subclasses import fake_tensor
from torch.fx.experimental import proxy_tensor
from torch.fx.passes import fake_tensor_prop
from torch.nn.utils import stateless
from torch.onnx import _type_utils
from torch.utils import _pytree


TORCH_ONNX_OPSET = onnxscript.values.Opset(domain="torch.onnx", version=1)


@onnxscript.script(opset=TORCH_ONNX_OPSET)
def prims_convert_element_type(tensor, dtype: int):
    return opset18.Cast(tensor, to=dtype)


@onnxscript.script(opset=TORCH_ONNX_OPSET)
def aten_getitem(self, i):
    # TODO(justinchuby): Support
    # i = opset18.Unsqueeze(i, opset18.Constant(value_ints=[0]))
    # return opset18.Gather(self, i, axis=0)
    return opset18.SequenceAt(self, i)


# A simple lookup table for atenlib functions
_ATENLIB_FUNCTIONS = {
    "getitem": aten_getitem,
    "prims::convert_element_type": prims_convert_element_type,
    "aten::abs": ops.core.aten_abs,
    "aten::acos": ops.core.aten_acos,
    "aten::acosh": ops.core.aten_acosh,
    "aten::add": ops.core.aten_add,
    "aten::addmm": ops.core.aten_addmm,
    "aten::amax": ops.core.aten_amax,
    "aten::amin": ops.core.aten_amin,
    # "aten::arange": ops.core.aten_arange_start_step,
    "aten::arange": ops.core.aten_arange_start,
    # "aten::arange": ops.core.aten_arange,
    "aten::asin": ops.core.aten_asin,
    "aten::asinh": ops.core.aten_asinh,
    "aten::atan": ops.core.aten_atan,
    "aten::atanh": ops.core.aten_atanh,
    "aten::bmm": ops.core.aten_bmm,
    "aten::ceil": ops.core.aten_ceil,
    "aten::clamp_max": ops.core.aten_clamp_max,
    "aten::clamp_min": ops.core.aten_clamp_min,
    "aten::clamp": ops.core.aten_clamp,
    "aten::clone": ops.core.aten_clone,
    "aten::cos": ops.core.aten_cos,
    "aten::cosh": ops.core.aten_cosh,
    "aten::detach": ops.core.aten_detach,
    "aten::div": ops.core.aten_div,
    "aten::dot": ops.core.aten_dot,
    "aten::empty": ops.core.aten_empty,
    "aten::empty_like": ops.core.aten_empty_like,
    "aten::eq": ops.core.aten_eq,
    "aten::equal": ops.core.aten_equal,
    "aten::exp": ops.core.aten_exp,
    "aten::exp2": ops.core.aten_exp2,
    "aten::expand": ops.core.aten_expand,
    "aten::erf": ops.core.aten_erf,
    "aten::fmod": ops.core.aten_fmod,
    "aten::full": ops.core.aten_full,
    "aten::full_like": ops.core.aten_full_like,
    "aten::ge": ops.core.aten_ge,
    "aten::gt": ops.core.aten_gt,
    "aten::isinf": ops.core.aten_isinf,
    "aten::log": ops.core.aten_log,
    "aten::le": ops.core.aten_le,
    "aten::log10": ops.core.aten_log10,
    "aten::log1p": ops.core.aten_log1p,
    "aten::log_softmax": ops.special.aten_special_log_softmax,
    "aten::log2": ops.core.aten_log2,
    "aten::logaddexp": ops.core.aten_logaddexp,
    "aten::logaddexp2": ops.core.aten_logaddexp2,
    "aten::logcumsumexp": ops.core.aten_logcumsumexp,
    "aten::logdet": ops.core.aten_logdet,
    "aten::logsumexp": ops.core.aten_logsumexp,
    "aten::lt": ops.core.aten_lt,
    "aten::matmul": ops.core.aten_matmul,
    "aten::maximum": ops.core.aten_maximum,
    "aten::minimum": ops.core.aten_minimum,
    "aten::mm": ops.core.aten_mm,
    "aten::mul": ops.core.aten_mul,
    "aten::ne": ops.core.aten_ne,
    "aten::neg": ops.core.aten_neg,
    "aten::new_full": ops.core.aten_new_full,
    "aten::adaptive_avg_pool1d": ops.nn.aten_adaptive_avg_pool1d,
    "aten::adaptive_avg_pool2d": ops.nn.aten_adaptive_avg_pool2d,
    "aten::adaptive_avg_pool3d": ops.nn.aten_adaptive_avg_pool3d,
    "aten::celu": ops.nn.aten_celu,
    "aten::elu": ops.nn.aten_elu,
    "aten::embedding": ops.core.aten_embedding,
    "aten::gelu": ops.nn.aten_gelu,
    "aten::leaky_relu": ops.nn.aten_leaky_relu,
    "aten::linear": ops.nn.aten_linear,
    "aten::logsigmoid": ops.nn.aten_log_sigmoid,
    "aten::relu": ops.nn.aten_relu,
    "aten::relu6": ops.nn.aten_relu6,
    "aten::selu": ops.core.aten_selu,
    "aten::upsample_nearest2d": ops.nn.aten_upsample_nearest2d,
    "aten::nonzero": ops.core.aten_nonzero,
    "aten::ones_like": ops.core.aten_ones_like,
    "aten::ones": ops.core.aten_ones,
    "aten::permute": ops.core.aten_permute,
    "aten::pow": ops.core.aten_pow,
    "aten::reciprocal": ops.core.aten_reciprocal,
    "aten::remainder": ops.core.aten_remainder,
    "aten::repeat": ops.core.aten_repeat,
    "aten::reshape": ops.core.aten_reshape,
    "aten::round": ops.core.aten_round,
    "aten::rsqrt": ops.core.aten_rsqrt,
    "aten::rsub": ops.core.aten_rsub,
    "aten::sigmoid": ops.core.aten_sigmoid,
    "aten::sign": ops.core.aten_sign,
    "aten::sin": ops.core.aten_sin,
    "aten::sinh": ops.core.aten_sinh,
    "aten::slice": ops.core.aten_slice,
    "aten::softmax": ops.special.aten_special_softmax,
    "aten::split": ops.core.aten_split,
    "aten::sqrt": ops.core.aten_sqrt,
    "aten::sub": ops.core.aten_sub,
    "aten::t": ops.core.aten_t,
    "aten::tan": ops.core.aten_tan,
    "aten::tanh": ops.core.aten_tanh,
    "aten::topk": ops.core.aten_topk,
    "aten::unsqueeze": ops.core.aten_unsqueeze,
    "aten::view": ops.core.aten_view,
    "aten::where": ops.core.aten_where,
    "aten::xlogy": ops.special.aten_special_xlogy,
    "aten::zeros": ops.core.aten_zeros,
    "aten::zeros_like": ops.core.aten_zeros_like,
    "aten::native_layer_norm": ops.core.aten_native_layer_norm,
    "aten::transpose": ops.core.aten_transpose,
    "aten::sum": ops.core.aten_sum_dim_IntList,
}


def _create_op_overload_to_exporter_key_table() -> Dict[
    Union[torch._ops.OpOverload, Callable], str
]:
    # TODO(justinchuby): Improve how the table is constructed.
    table: Dict[Union[torch._ops.OpOverload, Callable], str] = {}

    for op_namespace in (torch.ops.aten, torch.ops.prims):
        for attr_name in dir(op_namespace):
            op_overload_packet = getattr(op_namespace, attr_name)

            if not isinstance(op_overload_packet, torch._ops.OpOverloadPacket):
                continue
            exporter_look_up_key = op_overload_packet._qualified_op_name
            if _ATENLIB_FUNCTIONS.get(exporter_look_up_key) is None:
                # This aten op doesn't have ONNX exporter.
                continue

            for overload_name in op_overload_packet.overloads():
                op_overload = getattr(op_overload_packet, overload_name)
                # This line maps torch.ops.aten.add.Tensor, torch.ops.aten.add.Scalar, torch.ops.aten.add.out, etc
                # to "aten::add". This means the exporter for "aten::add" is used for all overloads of "aten::add".
                # This is applied to all ops under torch.ops.aten.
                #
                # TODO(wechi): in the future, we might want to write individual exporter for each overload, if,
                # for example, they have different type promotion rules. If so, just map different overloads to
                # different exporter keys.

                table[op_overload] = op_overload_packet._qualified_op_name

    return table


# Dictionary that maps torch.ops.aten.* to exporter look up key; e.g.,
# _OP_OVERLOAD_TO_EXPORTER_KEY_TABLE[torch.add.Tensor] is "aten::add".
# In subsequent code, torch.ops.aten.add.Tensor's exporter is found by
# registration.registry.get_function_group("aten::add").
_OP_OVERLOAD_TO_EXPORTER_KEY_TABLE = _create_op_overload_to_exporter_key_table()


def _create_onnx_friendly_decomposition_table() -> Dict[
    torch._ops.OpOverload, Callable
]:
    decomposition_table: Dict[torch._ops.OpOverload, Callable] = {}
    for op_overload, decomp_fn in torch._decomp.decomposition_table.items():
        # Skip decomposition into "prim::*" ops, because they are not generally supported by ONNX.
        # Skip decomposition for op_overload as long as that op_overload has a corresponding ONNX exporter.
        if (
            "torch._refs" in decomp_fn.__module__
            or op_overload in _OP_OVERLOAD_TO_EXPORTER_KEY_TABLE
        ):
            continue
        decomposition_table[op_overload] = decomp_fn
    return decomposition_table


# This is a subset of PyTorch's built-in aten-to-aten decomposition. If an aten
# op (e.g., torch.ops.aten.add.Tensor) has exporter, we exclude the op's decomposition
# function in the _ONNX_FRIENDLY_DECOMPOSITION_TABLE.
_ONNX_FRIENDLY_DECOMPOSITION_TABLE = _create_onnx_friendly_decomposition_table()


def _retrieve_or_adapt_input(fx_node_arg, fx_name_to_onnxscipt_value):
    """Map FX value to TorchScript value.

    When creating TorchScript graph from FX graph, we need a mapping from FX variable
    to TorchScript variable. This function maps FX variable, fx_node_arg, to torch.jit.Value.
    """

    onnx_tensor = fx_node_arg
    if isinstance(onnx_tensor, torch.fx.Node):
        # 1. fx_node_arg is a torch.fx.Node, which means
        #    fx_node_arg stands for the output of that torch.fx.Node.
        # 2. fx_node_arg (variable in torch.fx.Graph) is be mapped to
        #    torch.jit.Value, fx_name_to_onnxscipt_value[fx_node_arg.name],
        #    in TorchScript graph.
        onnx_tensor = fx_name_to_onnxscipt_value[onnx_tensor.name]
    elif isinstance(onnx_tensor, torch.dtype):
        onnx_tensor = int(_type_utils.JitScalarType.from_dtype(onnx_tensor).onnx_type())

    return onnx_tensor


def _filter_incompatible_kwargs(kwargs):
    """Filter out kwargs that are not supported by onnxscript."""
    filtered = {}
    for key, value in kwargs.items():
        if key in {
            "layout",
            "device",
            "requires_grad",
            "pin_memory",
            "memory_format",
        }:
            continue
        if key == "dtype":
            if value is None:
                filtered["dtype"] = -1
            else:
                filtered["dtype"] = int(
                    _type_utils.JitScalarType.from_dtype(value).onnx_type()
                )
            continue
        filtered[key] = value
    return filtered


def _wrap_fx_args_as_onnxscript_args(
    node, fx_name_to_onnxscipt_value
) -> Tuple[tuple, dict]:
    """Map all FX arguments of a node to arguments in TorchScript graph."""

    # This function assumes the order of arguments in FX op is the
    # same as the order of arguments in TorchScript op.
    # (1) Complete the arguments with default values.
    complete_args = []
    complete_kwargs = {}
    if inspect.isbuiltin(node.target):
        complete_args = node.args
    else:
        for i, expected_arg in enumerate(node.target._schema.arguments):
            if i < len(node.args):
                complete_args.append(node.args[i])
            else:
                if expected_arg.name in node.kwargs:
                    complete_kwargs[expected_arg.name] = node.kwargs[expected_arg.name]
                else:
                    # Get default from schema.
                    complete_kwargs[expected_arg.name] = expected_arg.default_value

    return (
        tuple(
            # (2) retrieve existing
            _retrieve_or_adapt_input(
                arg,
                fx_name_to_onnxscipt_value,
                # The node.meta["val"] is generated by FakeTensorProp.
                # node.meta["val"],
            )
            for arg in complete_args
        ),
        _filter_incompatible_kwargs(complete_kwargs),
    )


def _fill_tensor_meta(onnxscript_values, name: str, expected_values):
    """Fill the meta information of onnxscript_values with that from the fx FakeTensor."""
    flat_onnxscript_values, _ = _pytree.tree_flatten(onnxscript_values)
    flat_expected_values, _ = _pytree.tree_flatten(expected_values)
    for i, (onnxscript_value, expected_value) in enumerate(
        zip(flat_onnxscript_values, flat_expected_values)
    ):
        # Only set shape for now as we don't need type information.
        onnxscript_value.shape = tuple(expected_value.size())
        if i > 0:
            onnxscript_value.name = f"{name}_{i}"
        else:
            onnxscript_value.name = name


def _export_fx_to_onnxscript(fx_module_with_metadata, opset_version):

    # Initialize the ONNX graph
    onnxscript_graph = graph_building.TorchScriptGraph()
    tracer = graph_building.TorchScriptTracingEvaluator(onnxscript_graph)

    # In the following loop, a TorchScript graph is created to
    # represent the input FX graph with ONNX symbols (e.g., onnx::add).
    # To connect the values to nodes in the TorchScript graph, we maintian
    # fx_name_to_onnxscipt_value. Basically, we want to translate
    #   fx_tensor_x (type: torch.fx.Node) -> fx_node_1 -> fx_tensor_y (type: torch.fx.Node)
    # to
    #   fx_name_to_onnxscipt_value[fx_tensor_x.name] -> onnx_node_1 -> fx_name_to_onnxscipt_value[fx_tensor_y.name]
    fx_name_to_onnxscipt_value: Dict[
        str, Union[torch._C.Value, Tuple[torch._C.Value, ...]]
    ] = {}
    # Similar to fx_name_to_onnxscipt_value, we need a mapping fo real tensors (usually tensor parameters
    # in nn.Module). Note that TorchScript's cannot store real tensors; TorchScript values are all
    # symbolic. This is passed into ONNX ModelProto as the initializers.
    onnxscript_value_name_to_real_tensor: Dict[
        str, Union[torch.Tensor, Tuple[torch._C.Value, ...]]
    ] = {}
    # fx_module_with_metadata.print_readable()
    for node in fx_module_with_metadata.graph.nodes:
        if node.op == "placeholder":
            # Input of graph.
            output = onnxscript_graph.add_input(
                input_name=node.name, input_value=node.meta["val"]
            )
            assert (
                output is not None
            ), f"Node creates None with target={node.target} and name={node.name}"
            assert isinstance(output, graph_building.TorchScriptTensor)
            assert isinstance(output, onnxscript.tensor.Tensor)

            fx_name_to_onnxscipt_value[node.name] = output
        elif node.op == "call_function":
            # aten ops and other stateless functions.
            if node.target == operator.getitem and isinstance(
                fx_name_to_onnxscipt_value[node.args[0].name], tuple
            ):
                onnx_tensor_tuple = fx_name_to_onnxscipt_value[node.args[0].name]
                index = node.args[1]
                output = onnx_tensor_tuple[index]
                assert (
                    output is not None
                ), f"Node creates None with target={node.target} and name={node.name}"
                assert isinstance(
                    output, (graph_building.TorchScriptTensor, tuple)
                ), type(output)

                fx_name_to_onnxscipt_value[node.name] = output
                continue

            if node.target == operator.getitem:
                # __getitem__ on Tensor or Sequence of tensors. Not tuple.
                exporter_key = "getitem"
            elif (
                isinstance(node.target, torch._ops.OpOverload)
                and node.target in _OP_OVERLOAD_TO_EXPORTER_KEY_TABLE
            ):
                exporter_key = _OP_OVERLOAD_TO_EXPORTER_KEY_TABLE[node.target]
            else:
                raise RuntimeError(f"Unknown call_function target: {node.target}")
            # only latest opset version is only supported in atenlib for now
            print("function:", exporter_key)
            symbolic_fn = _ATENLIB_FUNCTIONS.get(exporter_key)
            assert symbolic_fn is not None, f"Cannot find function for {exporter_key}"
            # Map FX inputs to ONNX inputs and fill optional inputs with default values.
            onnx_args, onnx_kwargs = _wrap_fx_args_as_onnxscript_args(
                node, fx_name_to_onnxscipt_value
            )
            with evaluator.default_as(tracer):
                output: Union[
                    graph_building.TorchScriptTensor,
                    Tuple[graph_building.TorchScriptTensor],
                ] = symbolic_fn(*onnx_args, **onnx_kwargs)
            assert (
                output is not None
            ), f"Node creates None with target={node.target}, name={node.name}, args={onnx_args}, kwargs={onnx_kwargs}"
            # TODO(justinchuby): Assign tensor name and add diagnostic information.
            # Assign type and shape obtained from FakeTensorProp.
            _fill_tensor_meta(output, node.name, node.meta["val"])
            # One fx node could produce multiple outputs (e.g., tuple of tensors); in
            # that case, v is a tuple of TorchScriptTensors.
            assert isinstance(output, (graph_building.TorchScriptTensor, tuple)), type(
                output
            )
            fx_name_to_onnxscipt_value[node.name] = output
            continue
        elif node.op == "output":

            if isinstance(node.args[0], torch.fx.Node):
                onnx_tensor_or_tensor_tuple = fx_name_to_onnxscipt_value[
                    node.args[0].name
                ]
                onnxscript_graph.register_outputs(onnx_tensor_or_tensor_tuple)
            else:
                # ONNX can't represent collection types (e.g., dictionary, tuple of tuple of
                # tensor, etc), we flatten the collection and register each element as output.
                flat_args, _ = _pytree.tree_flatten(node.args[0])
                for arg in flat_args:
                    assert isinstance(
                        arg, torch.fx.Node
                    ), f"arg must be a torch.fx.Node, not {type(arg)}"
                    onnx_tensor_or_tensor_tuple = fx_name_to_onnxscipt_value[arg.name]
                    onnxscript_graph.register_outputs(onnx_tensor_or_tensor_tuple)
        elif node.op == "call_method":
            # TODO(wechi): Support call_method.
            raise RuntimeError("call_method is not supported yet.")
        elif node.op == "call_module":
            # TODO(wechi): Support call_module.
            raise RuntimeError("call_module is not supported yet.")
        elif node.op == "get_attr":
            current_attr = fx_module_with_metadata
            sub_attr_names = node.target.split(".")
            # If node.targe is "conv.weight", the following loop first
            # assigns fx_module_with_metadata.conv to current_attr, and then
            # fx_module_with_metadata.conv.weight to current_attr.
            while sub_attr_names:
                sub_attr_name = sub_attr_names.pop(0)
                if not hasattr(current_attr, sub_attr_name):
                    raise ValueError(
                        f"Attribute {sub_attr_name} is not found in {current_attr}."
                    )
                current_attr = getattr(current_attr, sub_attr_name)

            input_ = onnxscript_graph.add_input(
                input_name=node.name, input_value=current_attr
            )
            assert isinstance(input_, graph_building.TorchScriptTensor)
            assert isinstance(input_, onnxscript.tensor.Tensor)
            fx_name_to_onnxscipt_value[node.name] = input_
            onnxscript_value_name_to_real_tensor[input_.name] = current_attr
        else:
            # TODO(wechi): Support get_attr, call_module, call_method.
            raise RuntimeError(f"Found node type not defined in torch.fx: {node.op}")

    # Apply TorchScript's type promotion code.
    # Ideally, we should implement our type promotion but
    # to save time, we just reuse.
    onnxscript_graph.apply(
        torch._C._jit_pass_onnx_scalar_type_analysis,
        lowprecision_cast=True,
        opset_version=opset_version,
    )

    # When replace aten with onnx ops, the node-level shape type inference uses
    # ConstantValueMap which will not be cleared up until graph-level
    # shape type inference, and could be a bug. node/graph level inference should be both applied.
    # TODO(titaiwang): If onnx shape type inference is intended to be deprecated in converter.
    # node-level shape type inference should be also deprecated as well in g.op
    # if ONNX_GLOBALS.onnx_shape_inference:
    #     onnxscript_graph.apply(
    #         torch._C._jit_pass_onnx_graph_shape_type_inference,
    #         params_dict={},
    #         opset_version=opset_version,
    #     )

    return onnxscript_graph, onnxscript_value_name_to_real_tensor


def shape_inference_with_fake_tensor(decomposed_module: torch.fx.GraphModule, *args):
    # Use this mode to
    # 1. convert nn.Parameter's in nn.Module to FakeTensor
    # 2. run FakeTensorProp
    fake_tensor_mode = fake_tensor.FakeTensorMode()

    def to_fake_tensor(x):
        if isinstance(x, torch.Tensor) and not isinstance(x, fake_tensor.FakeTensor):
            return fake_tensor_mode.from_tensor(x)
        return x

    # "args" are FakeTensor in FakeTensorProp so the parameters and buffers
    # in model must be converted to FakeTensor as well.
    fake_parameters_and_buffers = {
        k: to_fake_tensor(v)
        for k, v in itertools.chain(
            decomposed_module.named_parameters(), decomposed_module.named_buffers()
        )
    }

    # Shape inference via FakeTensorProp
    with stateless._reparametrize_module(
        decomposed_module, fake_parameters_and_buffers
    ):
        # Assign output types and shapes to each node.
        # TODO(wechi): It's possible to get symbolic types (and shapes)
        # for each node's output. Consider to set "tracing_mode=symbolic"
        # when calling make_fx and then remove FakeTensorProp below.
        fake_tensor_prop.FakeTensorProp(decomposed_module, fake_tensor_mode).propagate(
            *args
        )

    return decomposed_module


def _export(
    module: torch.fx.GraphModule,
    opset_version=None,
    *args,
    decomposition_table: Optional[Dict[torch._ops.OpOverload, Callable]] = None,
    use_binary_format: bool = True,
):
    # Export FX graph to ONNX ModelProto.
    if decomposition_table is None:
        # Use default decomposition table.
        decomposition_table = torch._decomp.decomposition_table
    # Apply decomposition table to the input graph.
    decomposed_module = proxy_tensor.make_fx(module, decomposition_table)(*args)

    decomposed_module = shape_inference_with_fake_tensor(decomposed_module, *args)

    onnxscript_graph, initializers = _export_fx_to_onnxscript(
        decomposed_module, opset_version
    )
    print(onnxscript_graph.torch_graph)
    # Export TorchScript graph to ONNX ModelProto.
    onnx_model = onnxscript_graph.to_model_proto(initializers, opset_version)
    onnx.save_model(onnx_model, "gpt2.onnx")
    if use_binary_format:
        # Return ModelProto in binary format.
        return onnx_model.SerializeToString()
    # Return ModelProto
    return onnx_model


def export(
    fn: Union[torch.nn.Module, Callable],
    opset_version,
    *args,
    use_binary_format: bool = True,
):
    # args will be converted to symbolic tensor. Let's copy to avoid side effects.
    args = copy.deepcopy(args)
    # Translate callable to FX graph.
    #
    # TODO(wechi): There are several symbolic tracing mechanisms to convert
    # nn.Module to FX graph. We should choose the right one after they are
    # matured.
    graph_module, graph_guard = torch._dynamo.export(fn, *args, aten_graph=True)
    # Export FX graph to ONNX ModelProto.
    #
    # Note that ALL kwargs are folded into constants in graph_module, so we don't pass kwargs
    # to _export.
    return _export(
        graph_module,
        opset_version,
        *args,
        decomposition_table=_ONNX_FRIENDLY_DECOMPOSITION_TABLE,
        use_binary_format=use_binary_format,
    )


def export_without_kwargs(
    fn: Union[torch.nn.Module, Callable],
    opset_version,
    *args,
    use_binary_format: bool = True,
    **kwargs,
):
    if isinstance(fn, torch.nn.Module):
        signature = inspect.signature(fn.forward)
    else:
        signature = inspect.signature(fn)

    # We hope the input kwargs will be mapped to bound.args after binding.
    # If not, we will raise an error.
    bound = signature.bind(*args, **kwargs)
    bound.apply_defaults()
    # kwargs are not handled.
    assert not bound.kwargs

    class Wrapper(torch.nn.Module):
        def __init__(self, fn):
            super().__init__()
            self.fn = fn

        def forward(self, *args):
            result, _ = _pytree.tree_flatten(self.fn(*args))
            return result

    # args will be converted to symbolic tensor. Let's copy to avoid side effects.
    bound_args = copy.deepcopy(bound.args)
    # Translate callable to FX graph.
    #
    # TODO(wechi): There are several symbolic tracing mechanisms to convert
    # nn.Module to FX graph. We should choose the right one after they are
    # matured.

    class GraphCaptureCompiler:
        def __init__(self):
            self.captured_graph: Optional[torch.fx.GraphModule] = None
            self.captured_graph_count = 0

        def compile(self, gm: torch.fx.GraphModule, _):
            assert self.captured_graph_count == 0
            self.captured_graph = gm
            self.captured_graph_count += 1
            return gm

    compiler = GraphCaptureCompiler()
    torch._dynamo.optimize(compiler.compile, nopython=True)(Wrapper(fn))(*bound_args)
    torch._dynamo.reset()
    assert compiler.captured_graph
    # Export FX graph to ONNX ModelProto.
    return _export(
        compiler.captured_graph,
        opset_version,
        # Function optimized by _dynamo doesn't have None in args.
        *tuple(arg for arg in bound_args if arg is not None),
        decomposition_table=_ONNX_FRIENDLY_DECOMPOSITION_TABLE,
        use_binary_format=use_binary_format,
    )
